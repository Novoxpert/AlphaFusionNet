{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InMoSi2r8fAM",
        "outputId": "63deeea4-8800-408d-9a84-a5a2b7f4aaaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EKznW-DI9BNg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "\n",
        "merged_data = pd.read_pickle('/content/drive/MyDrive/data_btc_1min.pickle')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Indicators"
      ],
      "metadata": {
        "id": "u0zznDC4-qTA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YCAH6lxF9DYu"
      },
      "outputs": [],
      "source": [
        "merged_data['volume'] = merged_data['volume'].astype(float)\n",
        "merged_data['Volatility'] = merged_data['close'].pct_change(fill_method=None).rolling(5).std().fillna(0)\n",
        "\n",
        "# Calculate Moving Averages\n",
        "merged_data['SMA_5'] = merged_data['close'].rolling(window=5).mean()\n",
        "merged_data['EMA_5'] = merged_data['close'].ewm(span=5, adjust=False).mean()\n",
        "merged_data['SMA_10'] = merged_data['close'].rolling(window=10).mean()\n",
        "merged_data['EMA_10'] = merged_data['close'].ewm(span=10, adjust=False).mean()\n",
        "merged_data['SMA_20'] = merged_data['close'].rolling(window=20).mean()\n",
        "merged_data['EMA_20'] = merged_data['close'].ewm(span=20, adjust=False).mean()\n",
        "merged_data['EMA_50'] = merged_data['close'].ewm(span=50, adjust=False).mean()\n",
        "merged_data['SMA_100'] = merged_data['close'].rolling(window=100).mean()\n",
        "merged_data['EMA_100'] = merged_data['close'].ewm(span=100, adjust=False).mean()\n",
        "merged_data['EMA_200'] = merged_data['close'].ewm(span=200, adjust=False).mean()\n",
        "\n",
        "# Calculate Bollinger Bands\n",
        "merged_data['Std_20'] = merged_data['close'].rolling(window=20).std()\n",
        "merged_data['Upper_BB'] = merged_data['SMA_20'] + (2 * merged_data['Std_20'])\n",
        "merged_data['Lower_BB'] = merged_data['SMA_20'] - (2 * merged_data['Std_20'])\n",
        "merged_data['Daily_Returns'] = 100*(merged_data['close'].pct_change())\n",
        "\n",
        "# Add a Signal column (e.g., 1 if Close > SMA_20, else 0)\n",
        "# Calculate MACD and Signal Line\n",
        "# MACD Line = 12-period EMA - 26-period EMA\n",
        "merged_data['EMA_12'] = merged_data['close'].ewm(span=12, adjust=False).mean()\n",
        "merged_data['EMA_26'] = merged_data['close'].ewm(span=26, adjust=False).mean()\n",
        "merged_data['MACD'] = merged_data['EMA_12'] - merged_data['EMA_26']\n",
        "\n",
        "# Signal Line = 9-period EMA of MACD Line\n",
        "merged_data['Signal'] = merged_data['MACD'].ewm(span=9, adjust=False).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Target"
      ],
      "metadata": {
        "id": "SjRmHRGA-n-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "vr6YGPLu9GhP",
        "outputId": "a3d5c3e8-3796-4a9f-c079-f745de7ff7dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9998877673416994\n",
            "6.313087029411972e-05\n",
            "4.9101788006537555e-05\n",
            "0.5631694502704105\n",
            "0.43683054972958946\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "df = merged_data[['open', 'high', 'low', 'close', 'volume',\n",
        "       'quoteAssetVolume', 'numberOfTrades', 'takerBuyBaseVol',\n",
        "       'takerBuyQuoteVol', 'ignore', 'Volatility', 'SMA_5', 'EMA_5', 'SMA_10',\n",
        "       'EMA_10', 'SMA_20', 'EMA_20', 'EMA_50', 'SMA_100', 'EMA_100', 'EMA_200',\n",
        "       'Std_20', 'Upper_BB', 'Lower_BB', 'Daily_Returns', 'EMA_12', 'EMA_26',\n",
        "       'MACD', 'Signal']].copy()\n",
        "\n",
        "\n",
        "for x in df.columns:\n",
        "  df[x] = df[x].astype(float)\n",
        "\n",
        "list_cols = df.columns\n",
        "\n",
        "df['prev_return'] = 100*((df['close']-df['close'].shift(1))/df['close'])\n",
        "df['return'] = 100*df['close'].pct_change()\n",
        "df['volatility'] = 10*df['return'].rolling(30).std().shift(-30)\n",
        "# df['return'] = ((df['close'].shift(-1)-df['close'])/df['close'])\n",
        "df\n",
        "# df['return'] = (df['volume'].shift(1)+df['volume'].shift(2)+df['volume'].shift(3))\n",
        "# df['return'] = df['return']-df['return'].mean()\n",
        "# df['return'] = df['return']/df['return'].std()\n",
        "\n",
        "# df['return'] = 100*(df['close'].shift(-1440*2)-df['close'])/df['close']\n",
        "\n",
        "for x in list_cols:\n",
        "  tmp = df[x]-df[x].mean()\n",
        "  df[x] = tmp/tmp.std()\n",
        "\n",
        "# df['return'] = (df['volume'].shift(1)+df['volume'].shift(2)+df['volume'].shift(3))\n",
        "# df['return'] = (df['volume'].shift(1)-df['volume'].shift(2))\n",
        "# df['return'] = 20*(df['close'].shift(1)-df['close'].shift(2))\n",
        "# df['return'] = 20*(df['close'].shift(-1)-df['close'])\n",
        "# df['return'] = 100*(df['close'].shift(-5)-df['close'])/df['close']\n",
        "\n",
        "# df['return1'] = abs(100*((df['high'].rolling(5).max().shift(-5)-df['close'])/df['close']))\n",
        "# df['return2'] = abs(100*((df['low'].rolling(5).min().shift(-5)-df['close'])/df['close']))\n",
        "# df['return'] = df[['return1', 'return2']].max(axis=1)\n",
        "\n",
        "# df['return'] = 10*df['close'].rolling(15).std().shift(-15)\n",
        "\n",
        "\n",
        "# df['return'] = 100*(df['close'].shift(-1440*3)-df['close'])/df['close']\n",
        "# df['return'] = 100*(df['close'].shift(-1440*3)-df['close'])\n",
        "\n",
        "\n",
        "# df['return'] = ((df['volume'].shift(2)-df['volume'].shift(3))/df['volume'].shift(3))\n",
        "# df['return'] = (100*((df['close'].shift(-1440*7)-df['close'])/df['close'])).fillna(0)\n",
        "# df['return'] = 100*((df['volume'].shift(2)-df['volume'].shift(3))/df['volume'].shift(3))\n",
        "# df['return'] = (1*((df['volume'].shift(-1440)-df['volume'])/df['volume'])).fillna(0)\n",
        "# df['return2'] = (100*((df['close'].shift(-1)-df['close'])/df['close'])).fillna(0)\n",
        "\n",
        "\n",
        "df = df.fillna(0)\n",
        "\n",
        "threshold = 1\n",
        "df['return_class'] = 0\n",
        "df.loc[df['return']>threshold, 'return_class'] = 1\n",
        "df.loc[df['return']<-threshold, 'return_class'] = 2\n",
        "\n",
        "print((df['return_class']==0).sum()/len(df))\n",
        "print((df['return_class']==1).sum()/len(df))\n",
        "print((df['return_class']==2).sum()/len(df))\n",
        "\n",
        "threshold = 0.52\n",
        "df['return_class'] = 0\n",
        "df.loc[abs(df['volatility'])>threshold, 'return_class'] = 1\n",
        "# df.loc[df['return']<-threshold, 'return_class'] = 2\n",
        "\n",
        "print((df['return_class']==0).sum()/len(df))\n",
        "print((df['return_class']==1).sum()/len(df))\n",
        "# print((df['return_class']==2).sum()/len(df))\n",
        "\n",
        "df.iloc[:50000]['return_class'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YyVIojaE_1ft"
      },
      "outputs": [],
      "source": [
        "features = ['open', 'high', 'low', 'close', 'volume',\n",
        "       'quoteAssetVolume', 'numberOfTrades', 'takerBuyBaseVol',\n",
        "       'takerBuyQuoteVol', 'ignore', 'Volatility', 'SMA_5', 'EMA_5', 'SMA_10',\n",
        "       'EMA_10', 'SMA_20', 'EMA_20', 'EMA_50', 'SMA_100', 'EMA_100', 'EMA_200',\n",
        "       'Std_20', 'Upper_BB', 'Lower_BB', 'Daily_Returns', 'EMA_12', 'EMA_26',\n",
        "       'MACD', 'Signal', 'prev_return']\n",
        "\n",
        "selected_f_all = features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture"
      ],
      "metadata": {
        "id": "3--oVVS5-Eti"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmfiWzJC_k8U",
        "outputId": "e0235439-2c85-48db-d365-37482d74c21e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.fft\n",
        "\n",
        "configs = {\n",
        "    'task_name': 'classification',\n",
        "    'seq_len': 96,\n",
        "    'enc_in': len(features),\n",
        "    'd_model': 64,\n",
        "    'd_ff': 128,\n",
        "    'num_kernels': 3,\n",
        "    'dropout': 0.1,\n",
        "    'e_layers': 2,\n",
        "    'top_k': 2,\n",
        "    'num_class': 3\n",
        "}\n",
        "\n",
        "\n",
        "# Minimal versions of needed modules\n",
        "class Inception_Block_V1(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_kernels=3):\n",
        "        super().__init__()\n",
        "        kernel_sizes = [3, 5, 7][:num_kernels]\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=k, padding=k // 2)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sum(conv(x) for conv in self.convs) / len(self.convs)\n",
        "        return self.bn(out)\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.value_embedding = nn.Linear(input_dim, model_dim)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark_enc=None):\n",
        "        x = self.value_embedding(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "def FFT_for_Period(x, k=2):\n",
        "    xf = torch.fft.rfft(x, dim=1)\n",
        "    frequency_list = abs(xf).mean(0).mean(-1)\n",
        "    frequency_list[0] = 0\n",
        "    _, top_list = torch.topk(frequency_list, k)\n",
        "    top_list = top_list.detach().cpu().numpy()\n",
        "    period = x.shape[1] // top_list\n",
        "    return period, abs(xf).mean(-1)[:, top_list]\n",
        "\n",
        "class TimesBlock(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super().__init__()\n",
        "        self.seq_len = configs['seq_len']\n",
        "        self.k = configs['top_k']\n",
        "        self.conv = nn.Sequential(\n",
        "            Inception_Block_V1(configs['d_model'], configs['d_ff'], num_kernels=configs['num_kernels']),\n",
        "            nn.GELU(),\n",
        "            Inception_Block_V1(configs['d_ff'], configs['d_model'], num_kernels=configs['num_kernels'])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, N = x.size()\n",
        "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
        "        res = []\n",
        "        for i in range(self.k):\n",
        "            period = period_list[i]\n",
        "            length = self.seq_len\n",
        "            if length % period != 0:\n",
        "                pad_len = ((length // period) + 1) * period - length\n",
        "                x_padded = torch.cat([x, torch.zeros(B, pad_len, N).to(x.device)], dim=1)\n",
        "            else:\n",
        "                x_padded = x\n",
        "            out = x_padded.reshape(B, -1, period, N).permute(0, 3, 1, 2).contiguous()\n",
        "            out = self.conv(out)\n",
        "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
        "            res.append(out[:, :self.seq_len, :])\n",
        "        res = torch.stack(res, dim=-1)\n",
        "        period_weight = F.softmax(period_weight, dim=1).unsqueeze(1).unsqueeze(1).repeat(1, T, N, 1)\n",
        "        res = torch.sum(res * period_weight, -1)\n",
        "        return res + x\n",
        "\n",
        "class TimesNetClassifier(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super().__init__()\n",
        "        self.seq_len = configs['seq_len']\n",
        "        self.enc_embedding = DataEmbedding(configs['enc_in'], configs['d_model'], dropout=configs['dropout'])\n",
        "        self.layers = nn.ModuleList([TimesBlock(configs) for _ in range(configs['e_layers'])])\n",
        "        self.norm = nn.LayerNorm(configs['d_model'])\n",
        "        self.act = F.gelu\n",
        "        self.dropout = nn.Dropout(configs['dropout'])\n",
        "        self.projection = nn.Linear(configs['d_model'] * configs['seq_len'], configs['num_class'])\n",
        "\n",
        "    def forward(self, x, x_mask=None):\n",
        "        x = self.enc_embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = self.norm(layer(x))\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        if x_mask is not None:\n",
        "            x = x * x_mask.unsqueeze(-1)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.projection(x)\n",
        "\n",
        "\n",
        "class TimesNetBackbone(nn.Module):\n",
        "    def __init__(self, configs, embedding_dim=512):\n",
        "        super().__init__()\n",
        "        self.seq_len = configs['seq_len']\n",
        "        self.enc_embedding = DataEmbedding(configs['enc_in'], configs['d_model'], dropout=configs['dropout'])\n",
        "        self.layers = nn.ModuleList([TimesBlock(configs) for _ in range(configs['e_layers'])])\n",
        "        self.norm = nn.LayerNorm(configs['d_model'])\n",
        "        self.act = F.gelu\n",
        "        self.dropout = nn.Dropout(configs['dropout'])\n",
        "        self.embedding_proj = nn.Linear(configs['d_model'] * configs['seq_len'], embedding_dim)\n",
        "\n",
        "    def forward(self, x, x_mask=None):\n",
        "        x = self.enc_embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = self.norm(layer(x))\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        if x_mask is not None:\n",
        "            x = x * x_mask.unsqueeze(-1)\n",
        "        x = x.reshape(x.shape[0], -1)  # flatten\n",
        "        embedding = self.embedding_proj(x)  # shape: [B, embedding_dim]\n",
        "        return embedding\n",
        "\n",
        "class CombinedClassifier(nn.Module):\n",
        "    def __init__(self, backbone, extra_vector_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        total_dim = 512 + extra_vector_dim\n",
        "        self.classifier = nn.Linear(total_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, x_mask, extra_vector):\n",
        "        emb = self.backbone(x, x_mask)  # [B, 512]\n",
        "        # print(emb.shape)\n",
        "        combined = torch.cat([emb, extra_vector], dim=-1)  # [B, 512 + extra_dim]\n",
        "        return self.classifier(combined)\n",
        "\n",
        "\n",
        "configs['d_model'] = 64  # typical\n",
        "backbone = TimesNetBackbone(configs, embedding_dim=512)\n",
        "model = CombinedClassifier(backbone, extra_vector_dim=10, num_classes=3)\n",
        "\n",
        "x = torch.randn(32, configs['seq_len'], configs['enc_in'])      # input\n",
        "mask = torch.ones(32, configs['seq_len'])                       # mask\n",
        "extra = torch.randn(32, 10)                                     # extra features\n",
        "out = model(x, mask, extra)                                     # classification output\n",
        "print(out.shape)  # [32, 3]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqnWvbUTEoO_",
        "outputId": "c75b7b07-48b6-45ce-dac3-7a6efe93ff23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'task_name': 'classification',\n",
              " 'seq_len': 96,\n",
              " 'enc_in': 30,\n",
              " 'd_model': 64,\n",
              " 'd_ff': 128,\n",
              " 'num_kernels': 3,\n",
              " 'dropout': 0.1,\n",
              " 'e_layers': 2,\n",
              " 'top_k': 2,\n",
              " 'num_class': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ndceowUE2OY",
        "outputId": "4bca0cdd-f968-42b2-992c-e33fe0baa11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimesNetClassifier(\n",
            "  (enc_embedding): DataEmbedding(\n",
            "    (value_embedding): Linear(in_features=30, out_features=64, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x TimesBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Inception_Block_V1(\n",
            "          (convs): ModuleList(\n",
            "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (1): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "            (2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "          )\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Inception_Block_V1(\n",
            "          (convs): ModuleList(\n",
            "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (1): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "            (2): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "          )\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (projection): Linear(in_features=6144, out_features=3, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([32, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.fft\n",
        "\n",
        "# Minimal versions of needed modules\n",
        "class Inception_Block_V1(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_kernels=3):\n",
        "        super().__init__()\n",
        "        kernel_sizes = [3, 5, 7][:num_kernels]\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=k, padding=k // 2)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sum(conv(x) for conv in self.convs) / len(self.convs)\n",
        "        return self.bn(out)\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.value_embedding = nn.Linear(input_dim, model_dim)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark_enc=None):\n",
        "        x = self.value_embedding(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "def FFT_for_Period(x, k=2):\n",
        "    xf = torch.fft.rfft(x, dim=1)\n",
        "    frequency_list = abs(xf).mean(0).mean(-1)\n",
        "    frequency_list[0] = 0\n",
        "    _, top_list = torch.topk(frequency_list, k)\n",
        "    top_list = top_list.detach().cpu().numpy()\n",
        "    period = x.shape[1] // top_list\n",
        "    return period, abs(xf).mean(-1)[:, top_list]\n",
        "\n",
        "class TimesBlock(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super().__init__()\n",
        "        self.seq_len = configs['seq_len']\n",
        "        self.k = configs['top_k']\n",
        "        self.conv = nn.Sequential(\n",
        "            Inception_Block_V1(configs['d_model'], configs['d_ff'], num_kernels=configs['num_kernels']),\n",
        "            nn.GELU(),\n",
        "            Inception_Block_V1(configs['d_ff'], configs['d_model'], num_kernels=configs['num_kernels'])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, N = x.size()\n",
        "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
        "        res = []\n",
        "        for i in range(self.k):\n",
        "            period = period_list[i]\n",
        "            length = self.seq_len\n",
        "            if length % period != 0:\n",
        "                pad_len = ((length // period) + 1) * period - length\n",
        "                x_padded = torch.cat([x, torch.zeros(B, pad_len, N).to(x.device)], dim=1)\n",
        "            else:\n",
        "                x_padded = x\n",
        "            out = x_padded.reshape(B, -1, period, N).permute(0, 3, 1, 2).contiguous()\n",
        "            out = self.conv(out)\n",
        "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
        "            res.append(out[:, :self.seq_len, :])\n",
        "        res = torch.stack(res, dim=-1)\n",
        "        period_weight = F.softmax(period_weight, dim=1).unsqueeze(1).unsqueeze(1).repeat(1, T, N, 1)\n",
        "        res = torch.sum(res * period_weight, -1)\n",
        "        return res + x\n",
        "\n",
        "class TimesNetClassifier(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super().__init__()\n",
        "        self.seq_len = configs['seq_len']\n",
        "        self.enc_embedding = DataEmbedding(configs['enc_in'], configs['d_model'], dropout=configs['dropout'])\n",
        "        self.layers = nn.ModuleList([TimesBlock(configs) for _ in range(configs['e_layers'])])\n",
        "        self.norm = nn.LayerNorm(configs['d_model'])\n",
        "        self.act = F.gelu\n",
        "        self.dropout = nn.Dropout(configs['dropout'])\n",
        "        self.projection = nn.Linear(configs['d_model'] * configs['seq_len'], configs['num_class'])\n",
        "\n",
        "    def forward(self, x, x_mask=None):\n",
        "        x = self.enc_embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = self.norm(layer(x))\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        if x_mask is not None:\n",
        "            x = x * x_mask.unsqueeze(-1)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.projection(x)\n",
        "\n",
        "# Example configuration for classification\n",
        "# configs = {\n",
        "#     'task_name': 'classification',\n",
        "#     'seq_len': args.seq_len,\n",
        "#     'enc_in': args.enc_in,         # number of input features\n",
        "#     'd_model': args.d_model,\n",
        "#     'd_ff': args.d_ff,\n",
        "#     'num_kernels': args.num_kernels,\n",
        "#     'dropout': 0.1,\n",
        "#     'e_layers': args.e_layers,\n",
        "#     'top_k': args.top_k,\n",
        "#     'num_class': args.num_class       # number of classes\n",
        "# }\n",
        "\n",
        "# Instantiate and print model\n",
        "model = TimesNetClassifier(configs)\n",
        "print(model)\n",
        "\n",
        "# # Dummy input\n",
        "x = torch.randn(32, configs['seq_len'], configs['enc_in'])  # [batch, time, features]\n",
        "x_mask = torch.ones(32, configs['seq_len'])                 # [batch, time]\n",
        "output = model(x, x_mask)\n",
        "print(\"Output shape:\", output.shape)  # should be [32, num_class]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ug6GBW83E8vd"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TimesNetClassifier(configs).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset and Dataloader"
      ],
      "metadata": {
        "id": "UzVTJwk--adh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wVgGLWkxAB-t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ReturnClassDataset(Dataset):\n",
        "    def __init__(self, df, seq_len=96, features=None):\n",
        "        if features is None:\n",
        "            features = ['open', 'high', 'low', 'close', 'volume', 'return']\n",
        "        self.seq_len = seq_len\n",
        "        self.features = features\n",
        "\n",
        "        features = df[features].values  # all columns except last (assumed target)\n",
        "        targets = df['return_class'].values    # last column is the target\n",
        "\n",
        "        # Create input (X) and label (y)\n",
        "        X = []\n",
        "        y = []\n",
        "        for i in range(len(df) - seq_len):\n",
        "            X.append(features[i:i+seq_len])\n",
        "            y.append(targets[i + seq_len-1])\n",
        "\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x_mask = torch.ones(x.shape[0])\n",
        "        return x, x_mask, self.y[idx]\n",
        "\n",
        "\n",
        "# from dataset import generate_fake_ohlcv_dataset, ReturnClassDataset\n",
        "# from model import TimesNetClassifier  # assuming you save model in model.py\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "df_scaled = df.iloc[:50000].fillna(0)\n",
        "split = int(0.8 * len(df_scaled))\n",
        "df_train = df_scaled.iloc[:split]\n",
        "df_val = df_scaled.iloc[split:]\n",
        "\n",
        "train_dataset = ReturnClassDataset(df_train, seq_len=configs['seq_len'], features=features)\n",
        "val_dataset = ReturnClassDataset(df_val, seq_len=configs['seq_len'], features=features)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Train Model"
      ],
      "metadata": {
        "id": "zE1YzvHx-XZu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcgkLuxLAFQ8",
        "outputId": "016dcd72-0f1c-4d35-d122-ba4c00470013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training: 100%|██████████| 1247/1247 [00:26<00:00, 46.78it/s]\n",
            "Epoch 1 Validation: 100%|██████████| 310/310 [00:02<00:00, 121.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: Train Loss = 0.3177, Val Loss = 0.5099, Val Acc = 0.8164\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Natural       0.47      0.43      0.45      1737\n",
            "          Up       0.88      0.90      0.89      8167\n",
            "\n",
            "    accuracy                           0.82      9904\n",
            "   macro avg       0.68      0.67      0.67      9904\n",
            "weighted avg       0.81      0.82      0.81      9904\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Training: 100%|██████████| 1247/1247 [00:24<00:00, 51.40it/s]\n",
            "Epoch 2 Validation: 100%|██████████| 310/310 [00:02<00:00, 130.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: Train Loss = 0.2791, Val Loss = 0.7874, Val Acc = 0.7837\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Natural       0.23      0.10      0.14      1737\n",
            "          Up       0.83      0.93      0.88      8167\n",
            "\n",
            "    accuracy                           0.78      9904\n",
            "   macro avg       0.53      0.51      0.51      9904\n",
            "weighted avg       0.72      0.78      0.75      9904\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Training: 100%|██████████| 1247/1247 [00:24<00:00, 51.35it/s]\n",
            "Epoch 3 Validation: 100%|██████████| 310/310 [00:02<00:00, 130.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: Train Loss = 0.2602, Val Loss = 0.6829, Val Acc = 0.8081\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Natural       0.40      0.19      0.26      1737\n",
            "          Up       0.84      0.94      0.89      8167\n",
            "\n",
            "    accuracy                           0.81      9904\n",
            "   macro avg       0.62      0.56      0.57      9904\n",
            "weighted avg       0.77      0.81      0.78      9904\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Training: 100%|██████████| 1247/1247 [00:24<00:00, 51.16it/s]\n",
            "Epoch 4 Validation: 100%|██████████| 310/310 [00:02<00:00, 131.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: Train Loss = 0.2378, Val Loss = 0.9287, Val Acc = 0.7844\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Natural       0.32      0.21      0.26      1737\n",
            "          Up       0.84      0.91      0.87      8167\n",
            "\n",
            "    accuracy                           0.78      9904\n",
            "   macro avg       0.58      0.56      0.56      9904\n",
            "weighted avg       0.75      0.78      0.77      9904\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Training: 100%|██████████| 1247/1247 [00:24<00:00, 51.54it/s]\n",
            "Epoch 5 Validation: 100%|██████████| 310/310 [00:02<00:00, 127.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: Train Loss = 0.2169, Val Loss = 0.9520, Val Acc = 0.7919\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Natural       0.27      0.11      0.16      1737\n",
            "          Up       0.83      0.94      0.88      8167\n",
            "\n",
            "    accuracy                           0.79      9904\n",
            "   macro avg       0.55      0.52      0.52      9904\n",
            "weighted avg       0.73      0.79      0.75      9904\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Training: 100%|██████████| 1247/1247 [00:24<00:00, 51.33it/s]\n",
            "Epoch 6 Validation: 100%|██████████| 310/310 [00:02<00:00, 130.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: Train Loss = 0.1914, Val Loss = 0.7796, Val Acc = 0.7951\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Natural       0.42      0.42      0.42      1737\n",
            "          Up       0.88      0.88      0.88      8167\n",
            "\n",
            "    accuracy                           0.80      9904\n",
            "   macro avg       0.65      0.65      0.65      9904\n",
            "weighted avg       0.80      0.80      0.80      9904\n",
            "\n",
            "\n",
            "Early stopping triggered after 6 epochs. Best Val Acc = 0.8164\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "model = TimesNetClassifier(configs).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# Early stopping setup\n",
        "best_val_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "patience = 5\n",
        "best_model_state = None\n",
        "early_stop = False\n",
        "\n",
        "num_epochs = 50  # allow more epochs to benefit from early stopping\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for x, mask, y in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
        "        x, mask, y = x.to(device), mask.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x, mask)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, mask, y in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
        "            x, mask, y = x.to(device), mask.to(device), y.to(device)\n",
        "            output = model(x, mask)\n",
        "            loss = criterion(output, y)\n",
        "            val_losses.append(loss.item())\n",
        "            preds = output.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(all_targets, all_preds)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}: Train Loss = {sum(train_losses)/len(train_losses):.4f}, \"\n",
        "          f\"Val Loss = {sum(val_losses)/len(val_losses):.4f}, \"\n",
        "          f\"Val Acc = {val_acc:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(all_targets, all_preds, target_names=['Natural', 'Up']))\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = model.state_dict()\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs. Best Val Acc = {best_val_acc:.4f}\")\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "# Load best model weights\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}