{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNDsRrw7KpxUDSn6teFRnyj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/OHLCV/Time-Series-Library\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEEoZvXLhqjk","executionInfo":{"status":"ok","timestamp":1749559040204,"user_tz":-210,"elapsed":1481,"user":{"displayName":"yalda hematabadi","userId":"11120399924420094361"}},"outputId":"bf1505ca-d8cc-44aa-d012-45e145c5799c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/OHLCV/Time-Series-Library\n","btc_1min_embeddings.pkl       requirements.txt\n","checkpoints\t\t      result_long_term_forecast.txt\n","CONTRIBUTING.md\t\t      results\n","data_provider\t\t      run.py\n","dataset\t\t\t      scripts\n","Embedding_extractor.ipynb     test_btc_embeddings.pkl\n","embedding_to_csv_mapping.npy  test_embed_ohlcv.csv\n","exp\t\t\t      test_results\n","fixed_data_btc_1min1.csv      timesnet_embeddings_768.npy\n","fixed_data_btc_1min2.csv      timesnet_embeddings.pkl\n","layers\t\t\t      tsl\n","LICENSE\t\t\t      TSLib\n","models\t\t\t      tutorial\n","pic\t\t\t      utils\n","README.md\n"]}]},{"cell_type":"code","source":["!pip install reformer_pytorch\n","!pip install sktime"],"metadata":{"collapsed":true,"id":"4AaikPrriA_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iyj8tsPNheCW"},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import DataLoader\n","from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n","import argparse\n","import pickle"]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/Final_Block/test_embed_ohlcv.csv')\n","df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SndDihpMpHix","executionInfo":{"status":"ok","timestamp":1749558856246,"user_tz":-210,"elapsed":65,"user":{"displayName":"yalda hematabadi","userId":"11120399924420094361"}},"outputId":"14e72f57-6820-442a-8e33-376dee54b08f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 15978 entries, 0 to 15977\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   date        15978 non-null  object \n"," 1   open        15978 non-null  float64\n"," 2   high        15978 non-null  float64\n"," 3   low         15978 non-null  float64\n"," 4   close       15978 non-null  float64\n"," 5   volume      15978 non-null  float64\n"," 6   pre_return  15978 non-null  float64\n"," 7   return      15978 non-null  float64\n","dtypes: float64(7), object(1)\n","memory usage: 998.8+ KB\n"]}]},{"cell_type":"code","source":["import pickle\n","import argparse"],"metadata":{"id":"3EAyZtYQ1-jn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n","\n","# Extract backbone embeddings\n","def extract_timesnet_embeddings(checkpoint_path, data_config):\n","    class Args:\n","        def __init__(self, **kwargs):\n","            for k, v in kwargs.items():\n","                setattr(self, k, v)\n","\n","    args = Args(**data_config)\n","\n","    exp = Exp_Long_Term_Forecast(args)\n","\n","    if checkpoint_path and checkpoint_path != 'auto':\n","        exp.model.load_state_dict(torch.load(checkpoint_path))\n","        print(f\"Model loaded from {checkpoint_path}\")\n","\n","    exp.model.eval()\n","\n","    train_data, train_loader = exp._get_data(flag='test')\n","\n","    backbone_embeddings = []\n","\n","    def backbone_hook(module, input, output):\n","        \"\"\"after TimesBlock processing\"\"\"\n","        if isinstance(output, torch.Tensor):\n","            backbone_embeddings.append(output.detach().cpu().clone())\n","            print(f\"Captured backbone embeddings: {output.shape}\")\n","\n","    timesblock_handle = None\n","    for name, module in exp.model.named_modules():\n","        if name == 'model.0':\n","            timesblock_handle = module.register_forward_hook(backbone_hook)\n","            print(f\"Registered hook on TimesBlock: {name}\")\n","            break\n","\n","    if not timesblock_handle:\n","        print(\"TimesBlock not found, trying alternative approach...\")\n","        return extract_with_forward_modification(exp)\n","\n","    all_embeddings = []\n","    all_labels = []\n","\n","    print(f\"Processing {len(test_loader)} batches...\")\n","\n","    with torch.no_grad():\n","        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n","            batch_x = batch_x.float().to(exp.device)\n","            batch_y = batch_y.float().to(exp.device)\n","            batch_x_mark = batch_x_mark.float().to(exp.device)\n","            batch_y_mark = batch_y_mark.float().to(exp.device)\n","\n","            backbone_embeddings.clear()\n","\n","            output = exp.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n","\n","            if backbone_embeddings:\n","                embeddings = backbone_embeddings[0]\n","                all_embeddings.append(embeddings)\n","                all_labels.append(batch_y[:, -1:, :].cpu())\n","\n","                if i == 0:\n","                    print(f\"Backbone embeddings shape: {embeddings.shape}\")\n","                    print(f\"Model output shape: {output.shape}\")\n","\n","            if i % 20 == 0:\n","                print(f\"Processed {i}/{len(test_loader)} batches\")\n","\n","    if timesblock_handle:\n","        timesblock_handle.remove()\n","\n","    if all_embeddings:\n","        final_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n","        final_labels = torch.cat(all_labels, dim=0).numpy()\n","\n","        print(f\"\\n=== Extraction Complete ===\")\n","        print(f\"Final embeddings shape: {final_embeddings.shape}\")\n","        print(f\"Final labels shape: {final_labels.shape}\")\n","\n","        np.save('timesnet_backbone_embeddings.npy', final_embeddings)\n","        np.save('timesnet_labels.npy', final_labels)\n","        print(\"Embeddings saved to timesnet_backbone_embeddings.npy\")\n","        print(\"Labels saved to timesnet_labels.npy\")\n","\n","        return final_embeddings, final_labels\n","    else:\n","        print(\"No embeddings captured!\")\n","        return None, None\n","\n","\n","config = {\n","    'task_name': 'classification',\n","    'is_training': 0,\n","    'model': 'TimesNet',\n","    'data': 'custom',\n","    'root_path': '/content/drive/MyDrive/Final_Block/',\n","    'data_path': 'test_embed_ohlcv.csv',\n","    'features': 'MS',\n","    'target': 'return',\n","    'freq': 'h',\n","    'seasonal_patterns': 'Monthly',\n","    'inverse': 0,\n","    'seq_len': 48,\n","    'label_len': 24,\n","    'pred_len': 1,\n","    'e_layers': 1,\n","    'd_layers': 1,\n","    'factor': 3,\n","    'num_kernels': 6,\n","    'enc_in': 7,\n","    'dec_in': 7,\n","    'c_out': 1,\n","    'd_model': 768,\n","    'd_ff': 32,\n","    'top_k': 5,\n","    'des': 'QuickTest',\n","    'itr': 1,\n","    'batch_size': 32,\n","    'dropout': 0.1,\n","    'embed': 'timeF',\n","    'activation': 'gelu',\n","    'num_workers': 10,\n","    'patience': 3,\n","    'learning_rate': 0.0001,\n","    'use_gpu': True,\n","    'gpu_type': 'cuda',\n","    'use_multi_gpu': False,\n","    'gpu': 0,\n","    'data_split': '[0.0, 0.0, 1.0]',\n","    'num_class': 2\n","}\n","\n","\n","checkpoint_path = \"./checkpoints/long_term_forecast_test_btc_ohlcv_timesnet_TimesNet_custom_ftMS_sl48_ll24_pl1_dm64_nh8_el1_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_QuickTest_0/checkpoint.pth\"\n","\n","embeddings, labels = extract_timesnet_embeddings(checkpoint_path, config)"],"metadata":{"id":"VLBsti5a6WUy","executionInfo":{"status":"error","timestamp":1749559439003,"user_tz":-210,"elapsed":331,"user":{"displayName":"yalda hematabadi","userId":"11120399924420094361"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3af1c511-6709-4dd9-87f5-0261ffd9dacb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Use GPU: cuda:0\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Error(s) in loading state_dict for Model:\n\tUnexpected key(s) in state_dict: \"predict_linear.weight\", \"predict_linear.bias\". \n\tsize mismatch for model.0.conv.0.kernels.0.weight: copying a param with shape torch.Size([32, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 768, 1, 1]).\n\tsize mismatch for model.0.conv.0.kernels.1.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 768, 3, 3]).\n\tsize mismatch for model.0.conv.0.kernels.2.weight: copying a param with shape torch.Size([32, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([32, 768, 5, 5]).\n\tsize mismatch for model.0.conv.0.kernels.3.weight: copying a param with shape torch.Size([32, 64, 7, 7]) from checkpoint, the shape in current model is torch.Size([32, 768, 7, 7]).\n\tsize mismatch for model.0.conv.0.kernels.4.weight: copying a param with shape torch.Size([32, 64, 9, 9]) from checkpoint, the shape in current model is torch.Size([32, 768, 9, 9]).\n\tsize mismatch for model.0.conv.0.kernels.5.weight: copying a param with shape torch.Size([32, 64, 11, 11]) from checkpoint, the shape in current model is torch.Size([32, 768, 11, 11]).\n\tsize mismatch for model.0.conv.2.kernels.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 32, 1, 1]).\n\tsize mismatch for model.0.conv.2.kernels.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.0.conv.2.kernels.1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([768, 32, 3, 3]).\n\tsize mismatch for model.0.conv.2.kernels.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.0.conv.2.kernels.2.weight: copying a param with shape torch.Size([64, 32, 5, 5]) from checkpoint, the shape in current model is torch.Size([768, 32, 5, 5]).\n\tsize mismatch for model.0.conv.2.kernels.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.0.conv.2.kernels.3.weight: copying a param with shape torch.Size([64, 32, 7, 7]) from checkpoint, the shape in current model is torch.Size([768, 32, 7, 7]).\n\tsize mismatch for model.0.conv.2.kernels.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.0.conv.2.kernels.4.weight: copying a param with shape torch.Size([64, 32, 9, 9]) from checkpoint, the shape in current model is torch.Size([768, 32, 9, 9]).\n\tsize mismatch for model.0.conv.2.kernels.4.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.0.conv.2.kernels.5.weight: copying a param with shape torch.Size([64, 32, 11, 11]) from checkpoint, the shape in current model is torch.Size([768, 32, 11, 11]).\n\tsize mismatch for model.0.conv.2.kernels.5.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for enc_embedding.value_embedding.tokenConv.weight: copying a param with shape torch.Size([64, 12, 3]) from checkpoint, the shape in current model is torch.Size([768, 7, 3]).\n\tsize mismatch for enc_embedding.position_embedding.pe: copying a param with shape torch.Size([1, 5000, 64]) from checkpoint, the shape in current model is torch.Size([1, 5000, 768]).\n\tsize mismatch for enc_embedding.temporal_embedding.embed.weight: copying a param with shape torch.Size([64, 4]) from checkpoint, the shape in current model is torch.Size([768, 4]).\n\tsize mismatch for layer_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for layer_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for projection.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([2, 36864]).\n\tsize mismatch for projection.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-622019befe0c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./checkpoints/long_term_forecast_test_btc_ohlcv_timesnet_TimesNet_custom_ftMS_sl48_ll24_pl1_dm64_nh8_el1_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_QuickTest_0/checkpoint.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_timesnet_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-622019befe0c>\u001b[0m in \u001b[0;36mextract_timesnet_embeddings\u001b[0;34m(checkpoint_path, data_config)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model loaded from {checkpoint_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tUnexpected key(s) in state_dict: \"predict_linear.weight\", \"predict_linear.bias\". \n\tsize mismatch for model.0.conv.0.kernels.0.weight: copying a param with shape torch.Size([32, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 768, 1, 1]).\n\tsize mismatch for model.0.conv.0.kernels.1.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 768, 3, 3]).\n\tsize mismatch for model.0.conv.0.kernels.2.weight: copying a param with shape torch.Size([32, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([32, 768, 5, 5]).\n\tsize mismatch for model.0.conv.0.kernels.3.weight: copying a param with shape torch.Size([32, 64, 7, 7]) from checkpoint, the shape in current model is torch.Size([32, 768, 7, 7]).\n\tsize mismatch for model.0.conv.0.kernels.4.weight: copying a param with shape torch.Size([32, 64, 9, 9]) from checkpoint, the shape in current model is torch.Size([32, 768, 9, 9]).\n\tsize mismatch for model.0.conv.0.kernels.5.weight: copying a param with shape torch.Size([32, 64, 11, 11]) from checkpoint, the shape in current model is torch.Size([32, 768, 11, 11]).\n\tsize mismatch for model.0.conv.2.kernels.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 32, 1, 1]).\n\tsize mismatch for model.0.conv.2.kernels.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.0.conv.2.kernels.1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([768, 32, 3, 3]).\n\tsize mismatch for model.0.conv.2.kernels.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.0.conv.2.kernels.2.weight: copying a param with shape torch.Size([64, 32, 5, 5]) from checkpoint, the shape in current model is torch.Size([768, 32, 5, 5]).\n\tsize mismatch for model.0.conv.2.kernels.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.0.conv.2.kernels.3.weight: copying a param with shape torch.Size([64, 32, 7, 7]) from checkpoint, the shape in current model is torch.Size([768, 32, 7, 7]).\n\tsize mismatch for model.0.conv.2.kernels.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.0.conv.2.kernels.4.weight: copying a param with shape torch.Size([64, 32, 9, 9]) from checkpoint, the shape in current model is torch.Size([768, 32, 9, 9]).\n\tsize mismatch for model.0.conv.2.kernels.4.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.0.conv.2.kernels.5.weight: copying a param with shape torch.Size([64, 32, 11, 11]) from checkpoint, the shape in current model is torch.Size([768, 32, 11, 11]).\n\tsize mismatch for model.0.conv.2.kernels.5.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for enc_embedding.value_embedding.tokenConv.weight: copying a param with shape torch.Size([64, 12, 3]) from checkpoint, the shape in current model is torch.Size([768, 7, 3]).\n\tsize mismatch for enc_embedding.position_embedding.pe: copying a param with shape torch.Size([1, 5000, 64]) from checkpoint, the shape in current model is torch.Size([1, 5000, 768]).\n\tsize mismatch for enc_embedding.temporal_embedding.embed.weight: copying a param with shape torch.Size([64, 4]) from checkpoint, the shape in current model is torch.Size([768, 4]).\n\tsize mismatch for layer_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for layer_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for projection.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([2, 36864]).\n\tsize mismatch for projection.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2])."]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n","\n","def debug_model_structure(checkpoint_path, data_config):\n","    \"\"\"\n","    Debug the TimesNet model structure to find the right layers for embedding extraction\n","    \"\"\"\n","\n","    class Args:\n","        def __init__(self, **kwargs):\n","            for k, v in kwargs.items():\n","                setattr(self, k, v)\n","\n","    args = Args(**data_config)\n","\n","    exp = Exp_Long_Term_Forecast(args)\n","\n","    if checkpoint_path and checkpoint_path != 'auto':\n","        exp.model.load_state_dict(torch.load(checkpoint_path))\n","\n","    print(\"=== TimesNet Model Structure ===\")\n","    for name, module in exp.model.named_modules():\n","        print(f\"{name}: {type(module).__name__}\")\n","\n","    print(\"\\n=== Model Children ===\")\n","    for name, child in exp.model.named_children():\n","        print(f\"{name}: {type(child).__name__}\")\n","        if hasattr(child, '__len__') and not isinstance(child, torch.nn.Parameter):\n","            try:\n","                print(f\"  Length: {len(child)}\")\n","            except:\n","                pass\n","\n","    return exp\n","\n","def extract_embeddings_with_multiple_hooks(exp, target_layers=None):\n","    \"\"\"\n","    Extract embeddings by hooking into multiple potential layers\n","    \"\"\"\n","    if target_layers is None:\n","        target_layers = [\n","            'layer_stack',\n","            'encoder',\n","            'enc_layers',\n","            'backbone',\n","            'layers',\n","            'projection'\n","        ]\n","\n","    test_data, test_loader = exp._get_data(flag='test')\n","\n","    layer_embeddings = {}\n","\n","    def create_hook(layer_name):\n","        def hook_fn(module, input, output):\n","            if isinstance(output, torch.Tensor):\n","                layer_embeddings[layer_name] = output.detach().cpu().clone()\n","                print(f\"Captured from {layer_name}: {output.shape}\")\n","        return hook_fn\n","\n","    hook_handles = []\n","    for name, module in exp.model.named_modules():\n","        for target in target_layers:\n","            if target in name.lower():\n","                handle = module.register_forward_hook(create_hook(name))\n","                hook_handles.append(handle)\n","                print(f\"Registered hook on: {name}\")\n","\n","    if not hook_handles:\n","        print(\"No target layers found, hooking all direct children...\")\n","        for name, module in exp.model.named_children():\n","            handle = module.register_forward_hook(create_hook(name))\n","            hook_handles.append(handle)\n","            print(f\"Registered hook on child: {name}\")\n","\n","    exp.model.eval()\n","    all_layer_embeddings = {layer: [] for layer in layer_embeddings.keys()}\n","\n","    # Extract embeddings from one batch\n","    with torch.no_grad():\n","        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n","            if i > 0:\n","                break\n","\n","            batch_x = batch_x.float().to(exp.device)\n","            batch_y = batch_y.float().to(exp.device)\n","            batch_x_mark = batch_x_mark.float().to(exp.device)\n","            batch_y_mark = batch_y_mark.float().to(exp.device)\n","\n","            layer_embeddings.clear()\n","\n","            print(f\"\\nProcessing batch with shapes:\")\n","            print(f\"batch_x: {batch_x.shape}\")\n","            print(f\"batch_y: {batch_y.shape}\")\n","\n","            output = exp.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n","            print(f\"Model output shape: {output.shape}\")\n","\n","            print(f\"\\nCaptured embeddings from {len(layer_embeddings)} layers:\")\n","            for layer_name, embeddings in layer_embeddings.items():\n","                print(f\"  {layer_name}: {embeddings.shape}\")\n","\n","    for handle in hook_handles:\n","        handle.remove()\n","\n","    return layer_embeddings\n","\n","def extract_final_embeddings(exp, layer_name):\n","    \"\"\"\n","    Extract embeddings from a specific layer across all test data\n","    \"\"\"\n","    test_data, test_loader = exp._get_data(flag='test')\n","\n","    embeddings_list = []\n","\n","    def embedding_hook(module, input, output):\n","        if isinstance(output, torch.Tensor):\n","            embeddings_list.append(output.detach().cpu().clone())\n","\n","    hook_handle = None\n","    for name, module in exp.model.named_modules():\n","        if name == layer_name:\n","            hook_handle = module.register_forward_hook(embedding_hook)\n","            print(f\"Hooked onto layer: {name}\")\n","            break\n","\n","    if not hook_handle:\n","        print(f\"Layer {layer_name} not found!\")\n","        return None\n","\n","    exp.model.eval()\n","    all_embeddings = []\n","\n","    with torch.no_grad():\n","        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n","            batch_x = batch_x.float().to(exp.device)\n","            batch_y = batch_y.float().to(exp.device)\n","            batch_x_mark = batch_x_mark.float().to(exp.device)\n","            batch_y_mark = batch_y_mark.float().to(exp.device)\n","\n","            embeddings_list.clear()\n","\n","            _ = exp.model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n","\n","            if embeddings_list:\n","                all_embeddings.append(embeddings_list[0])\n","\n","            if i % 10 == 0:\n","                print(f\"Processed batch {i}/{len(test_loader)}\")\n","\n","    hook_handle.remove()\n","\n","    if all_embeddings:\n","        final_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n","        print(f\"Final embeddings shape: {final_embeddings.shape}\")\n","        return final_embeddings\n","    else:\n","        print(\"No embeddings captured!\")\n","        return None\n","\n","config = {\n","    'task_name': 'long_term_forecast',\n","    'is_training': 0,\n","    'model_id': 'test_btc_ohlcv_timesnet',\n","    'model': 'TimesNet',\n","    'data': 'custom',\n","    'root_path': './dataset/my_data/',\n","    'data_path': 'test_btc_ohlcv.csv',\n","    'features': 'MS',\n","    'target': 'return',\n","    'freq': 'h',\n","    'seasonal_patterns': 'Monthly',\n","    'inverse': 0,\n","    'seq_len': 48,\n","    'label_len': 24,\n","    'pred_len': 1,\n","    'e_layers': 1,\n","    'd_layers': 1,\n","    'factor': 3,\n","    'num_kernels': 6,\n","    'enc_in': 12,\n","    'dec_in': 12,\n","    'c_out': 1,\n","    'd_model': 64,\n","    'd_ff': 32,\n","    'top_k': 5,\n","    'des': 'QuickTest',\n","    'itr': 1,\n","    'batch_size': 32,\n","    'dropout': 0.1,\n","    'embed': 'timeF',\n","    'activation': 'gelu',\n","    'num_workers': 10,\n","    'patience': 3,\n","    'learning_rate': 0.0001,\n","    'use_gpu': True,\n","    'gpu_type': 'cuda',\n","    'use_multi_gpu': False,\n","    'gpu': 0\n","}\n","\n","print(\"Step 1: Analyzing model structure...\")\n","exp = debug_model_structure('auto', config)  #'auto' or checkpoint path\n","\n","print(\"\\nStep 2: Testing hooks on multiple layers...\")\n","layer_embeddings = extract_embeddings_with_multiple_hooks(exp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kDq2AgBo4lxt","executionInfo":{"status":"ok","timestamp":1749387170762,"user_tz":-210,"elapsed":1317,"user":{"displayName":"yalda hematabadi","userId":"11120399924420094361"}},"outputId":"53c73f40-a2e5-4238-de27-df21dc7d5433"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 1: Analyzing model structure...\n","Use GPU: cuda:0\n","=== TimesNet Model Structure ===\n",": Model\n","model: ModuleList\n","model.0: TimesBlock\n","model.0.conv: Sequential\n","model.0.conv.0: Inception_Block_V1\n","model.0.conv.0.kernels: ModuleList\n","model.0.conv.0.kernels.0: Conv2d\n","model.0.conv.0.kernels.1: Conv2d\n","model.0.conv.0.kernels.2: Conv2d\n","model.0.conv.0.kernels.3: Conv2d\n","model.0.conv.0.kernels.4: Conv2d\n","model.0.conv.0.kernels.5: Conv2d\n","model.0.conv.1: GELU\n","model.0.conv.2: Inception_Block_V1\n","model.0.conv.2.kernels: ModuleList\n","model.0.conv.2.kernels.0: Conv2d\n","model.0.conv.2.kernels.1: Conv2d\n","model.0.conv.2.kernels.2: Conv2d\n","model.0.conv.2.kernels.3: Conv2d\n","model.0.conv.2.kernels.4: Conv2d\n","model.0.conv.2.kernels.5: Conv2d\n","enc_embedding: DataEmbedding\n","enc_embedding.value_embedding: TokenEmbedding\n","enc_embedding.value_embedding.tokenConv: Conv1d\n","enc_embedding.position_embedding: PositionalEmbedding\n","enc_embedding.temporal_embedding: TimeFeatureEmbedding\n","enc_embedding.temporal_embedding.embed: Linear\n","enc_embedding.dropout: Dropout\n","layer_norm: LayerNorm\n","predict_linear: Linear\n","projection: Linear\n","\n","=== Model Children ===\n","model: ModuleList\n","  Length: 1\n","enc_embedding: DataEmbedding\n","layer_norm: LayerNorm\n","predict_linear: Linear\n","projection: Linear\n","\n","Step 2: Testing hooks on multiple layers...\n","test 658\n","Registered hook on: projection\n","\n","Processing batch with shapes:\n","batch_x: torch.Size([32, 48, 12])\n","batch_y: torch.Size([32, 25, 12])\n","Captured from projection: torch.Size([32, 49, 1])\n","Model output shape: torch.Size([32, 1, 12])\n","\n","Captured embeddings from 1 layers:\n","  projection: torch.Size([32, 49, 1])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"8rRwSsrqu5Rh"},"execution_count":null,"outputs":[]}]}